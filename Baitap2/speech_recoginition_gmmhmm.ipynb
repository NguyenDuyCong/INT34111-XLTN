{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path) # read .wav file\n",
    "    hop_length = math.floor(sr*0.010) # 10ms hop\n",
    "    win_length = math.floor(sr*0.025) # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1,1)) \n",
    "    # energy feature\n",
    "    rms = librosa.feature.rms(y, hop_length=hop_length)\n",
    "    frame_feature = np.concatenate([mfcc, rms], axis=0)\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(frame_feature, order=1, mode='nearest')\n",
    "    delta2 = librosa.feature.delta(frame_feature, order=2, mode='nearest')\n",
    "    # X is 39 x T\n",
    "    X = np.concatenate([frame_feature, delta1, delta2], axis=0) # O^r\n",
    "    # return T x 39 (transpose of X)\n",
    "    return X.T # hmmlearn use T x N matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = get_mfcc(\"./data/duong/17.wav\")\n",
    "# mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir,f)) for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load hai dataset\n",
      "Load tien dataset\n",
      "Load duong dataset\n",
      "Load y_te dataset\n",
      "Load benh_nhan dataset\n",
      "Load test_hai dataset\n",
      "Load test_tien dataset\n",
      "Load test_duong dataset\n",
      "Load test_y_te dataset\n",
      "Load test_benh_nhan dataset\n",
      "vectors (24281, 39)\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"hai\", \"tien\", \"duong\", \"y_te\", \"benh_nhan\", \"test_hai\", \"test_tien\", \"test_duong\", \"test_y_te\", \"test_benh_nhan\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)\n",
    "# Run K-Means algorithm to get clusters\n",
    "# kmeans = clustering(all_vectors)\n",
    "# print(\"centers\", kmeans.cluster_centers_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initByBakis(nStates, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nStates)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nStates, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "    \n",
    "def getTransmatPrior(nStates, bakisLevel):\n",
    "    ''' get transmat prior '''\n",
    "    transmatPrior = (1. / bakisLevel) * np.eye(nStates)\n",
    "\n",
    "    for i in range(nStates - (bakisLevel - 1)):\n",
    "        for j in range(bakisLevel - 1):\n",
    "            transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "\n",
    "    for i in range(nStates - bakisLevel + 1, nStates):\n",
    "        for j in range(nStates - i -j):\n",
    "            transmatPrior[i, i + j] = 1. / (nStates - i)\n",
    "\n",
    "    return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class hai\n",
      "(2107, 39) [19, 26, 15, 26, 23, 20, 16, 24, 33, 21, 15, 19, 20, 19, 23, 18, 15, 25, 23, 20, 19, 24, 16, 16, 22, 27, 27, 14, 14, 18, 20, 34, 13, 13, 13, 21, 14, 19, 16, 18, 30, 15, 11, 16, 27, 17, 16, 28, 14, 26, 14, 33, 40, 24, 22, 14, 28, 17, 12, 21, 18, 32, 26, 17, 21, 17, 13, 17, 18, 13, 8, 16, 12, 15, 14, 10, 28, 18, 12, 127, 106, 114, 93, 119] 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -183140.0968             +nan\n",
      "         2     -161176.2523      +21963.8445\n",
      "         3     -156305.8696       +4870.3827\n",
      "         4     -155049.7597       +1256.1099\n",
      "         5     -154466.6621        +583.0976\n",
      "         6     -154040.8564        +425.8057\n",
      "         7     -153853.6652        +187.1912\n",
      "         8     -153750.3868        +103.2784\n",
      "         9     -153649.7051        +100.6817\n",
      "        10     -153543.6955        +106.0096\n",
      "        11     -153327.8316        +215.8639\n",
      "        12     -152826.9969        +500.8347\n",
      "        13     -152670.5408        +156.4560\n",
      "        14     -152531.9012        +138.6396\n",
      "        15     -152454.4569         +77.4443\n",
      "        16     -152419.9919         +34.4650\n",
      "        17     -152401.4955         +18.4965\n",
      "        18     -152384.5753         +16.9202\n",
      "        19     -152368.4516         +16.1237\n",
      "        20     -152355.6616         +12.7899\n",
      "        21     -152335.0820         +20.5797\n",
      "        22     -152297.5252         +37.5568\n",
      "        23     -152271.3613         +26.1639\n",
      "        24     -152266.6577          +4.7036\n",
      "        25     -152263.9825          +2.6752\n",
      "        26     -152258.4703          +5.5122\n",
      "        27     -152246.3568         +12.1135\n",
      "        28     -152233.9425         +12.4143\n",
      "        29     -152212.0284         +21.9141\n",
      "        30     -152183.0703         +28.9582\n",
      "        31     -152166.0009         +17.0694\n",
      "        32     -152139.6620         +26.3389\n",
      "        33     -152109.8901         +29.7718\n",
      "        34     -152073.4481         +36.4420\n",
      "        35     -152006.0936         +67.3545\n",
      "        36     -151933.7048         +72.3888\n",
      "        37     -151870.2003         +63.5045\n",
      "        38     -151833.0848         +37.1156\n",
      "        39     -151816.7761         +16.3087\n",
      "        40     -151811.1953          +5.5807\n",
      "        41     -151808.9480          +2.2473\n",
      "        42     -151807.9702          +0.9778\n",
      "        43     -151807.5141          +0.4561\n",
      "        44     -151807.2817          +0.2323\n",
      "        45     -151807.0789          +0.2029\n",
      "        46     -151806.7028          +0.3761\n",
      "        47     -151804.0306          +2.6722\n",
      "        48     -151800.5942          +3.4364\n",
      "        49     -151799.6710          +0.9232\n",
      "        50     -151798.3908          +1.2802\n",
      "        51     -151793.2230          +5.1678\n",
      "        52     -151789.7787          +3.4443\n",
      "        53     -151789.6208          +0.1578\n",
      "        54     -151789.5619          +0.0589\n",
      "        55     -151789.5267          +0.0352\n",
      "        56     -151789.5001          +0.0267\n",
      "        57     -151789.4769          +0.0232\n",
      "        58     -151789.4551          +0.0219\n",
      "        59     -151789.4334          +0.0217\n",
      "        60     -151789.4112          +0.0222\n",
      "        61     -151789.3882          +0.0231\n",
      "        62     -151789.3640          +0.0242\n",
      "        63     -151789.3387          +0.0253\n",
      "        64     -151789.3122          +0.0264\n",
      "        65     -151789.2849          +0.0273\n",
      "        66     -151789.2571          +0.0278\n",
      "        67     -151789.2292          +0.0279\n",
      "        68     -151789.2016          +0.0275\n",
      "        69     -151789.1749          +0.0267\n",
      "        70     -151789.1494          +0.0255\n",
      "        71     -151789.1254          +0.0240\n",
      "        72     -151789.1029          +0.0225\n",
      "        73     -151789.0821          +0.0208\n",
      "        74     -151789.0629          +0.0192\n",
      "        75     -151789.0452          +0.0177\n",
      "        76     -151789.0289          +0.0163\n",
      "        77     -151789.0138          +0.0151\n",
      "        78     -151788.9999          +0.0139\n",
      "        79     -151788.9869          +0.0130\n",
      "        80     -151788.9748          +0.0121\n",
      "        81     -151788.9634          +0.0114\n",
      "        82     -151788.9525          +0.0109\n",
      "        83     -151788.9420          +0.0105\n",
      "        84     -151788.9318          +0.0102\n",
      "        85     -151788.9217          +0.0101\n",
      "        86     -151788.9115          +0.0102\n",
      "        87     -151788.9010          +0.0105\n",
      "        88     -151788.8898          +0.0111\n",
      "        89     -151788.8776          +0.0122\n",
      "        90     -151788.8637          +0.0139\n",
      "        91     -151788.8471          +0.0166\n",
      "        92     -151788.8261          +0.0210\n",
      "        93     -151788.7975          +0.0286\n",
      "        94     -151788.7550          +0.0425\n",
      "        95     -151788.6846          +0.0703\n",
      "        96     -151788.5526          +0.1320\n",
      "        97     -151788.2641          +0.2886\n",
      "        98     -151787.3791          +0.8850\n",
      "        99     -151781.5523          +5.8268\n",
      "       100     -151776.0219          +5.5304\n",
      "       101     -151774.3361          +1.6858\n",
      "       102     -151772.2826          +2.0535\n",
      "       103     -151769.8310          +2.4516\n",
      "       104     -151767.5594          +2.2716\n",
      "       105     -151765.7065          +1.8529\n",
      "       106     -151764.1009          +1.6057\n",
      "       107     -151762.5732          +1.5277\n",
      "       108     -151761.1556          +1.4176\n",
      "       109     -151759.9940          +1.1616\n",
      "       110     -151759.1229          +0.8711\n",
      "       111     -151758.4673          +0.6556\n",
      "       112     -151757.9551          +0.5122\n",
      "       113     -151757.5301          +0.4250\n",
      "       114     -151757.1344          +0.3956\n",
      "       115     -151756.7448          +0.3897\n",
      "       116     -151756.4442          +0.3006\n",
      "       117     -151756.2719          +0.1723\n",
      "       118     -151756.1633          +0.1086\n",
      "       119     -151756.0702          +0.0931\n",
      "       120     -151755.9574          +0.1128\n",
      "       121     -151755.7916          +0.1657\n",
      "       122     -151755.5934          +0.1982\n",
      "       123     -151755.4562          +0.1372\n",
      "       124     -151755.3939          +0.0623\n",
      "       125     -151755.3649          +0.0290\n",
      "       126     -151755.3485          +0.0164\n",
      "       127     -151755.3377          +0.0108\n",
      "       128     -151755.3299          +0.0078\n"
     ]
    }
   ],
   "source": [
    "cname = \"hai\"\n",
    "n_coms = 9\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mst',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class tien\n",
      "(3602, 39) [13, 14, 18, 19, 16, 18, 24, 21, 16, 20, 17, 24, 18, 26, 19, 16, 16, 29, 24, 28, 19, 36, 25, 26, 29, 24, 23, 31, 29, 25, 14, 24, 18, 18, 19, 18, 16, 19, 24, 16, 19, 13, 21, 55, 17, 28, 25, 20, 41, 23, 24, 20, 14, 13, 24, 29, 41, 26, 26, 36, 25, 18, 25, 18, 28, 18, 19, 19, 24, 55, 29, 26, 24, 21, 16, 31, 24, 19, 17, 16, 28, 25, 23, 16, 104, 104, 99, 109, 109, 112, 112, 114, 124, 106, 117, 112, 124, 114, 114] 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -309588.5653             +nan\n",
      "         2     -255721.7575      +53866.8078\n",
      "         3     -247643.0089       +8078.7486\n",
      "         4     -244260.8135       +3382.1954\n",
      "         5     -242413.5826       +1847.2309\n",
      "         6     -241128.9033       +1284.6793\n",
      "         7     -240464.2950        +664.6083\n",
      "         8     -240065.9266        +398.3685\n",
      "         9     -239815.5907        +250.3359\n",
      "        10     -239616.9569        +198.6338\n",
      "        11     -239521.4459         +95.5110\n",
      "        12     -239440.3787         +81.0671\n",
      "        13     -239379.2842         +61.0945\n",
      "        14     -239340.9585         +38.3257\n",
      "        15     -239330.4313         +10.5272\n",
      "        16     -239324.4729          +5.9584\n",
      "        17     -239316.6683          +7.8047\n",
      "        18     -239308.9967          +7.6715\n",
      "        19     -239301.4182          +7.5786\n",
      "        20     -239286.7829         +14.6353\n",
      "        21     -239264.3837         +22.3992\n",
      "        22     -239242.8571         +21.5266\n",
      "        23     -239223.3904         +19.4667\n",
      "        24     -239198.0634         +25.3270\n",
      "        25     -239177.2653         +20.7982\n",
      "        26     -239171.7561          +5.5091\n",
      "        27     -239167.8633          +3.8928\n",
      "        28     -239157.6890         +10.1744\n",
      "        29     -239144.5508         +13.1382\n",
      "        30     -239137.1010          +7.4498\n",
      "        31     -239136.1362          +0.9648\n",
      "        32     -239135.2513          +0.8849\n",
      "        33     -239134.3485          +0.9029\n",
      "        34     -239133.6603          +0.6881\n",
      "        35     -239133.1993          +0.4611\n",
      "        36     -239132.8041          +0.3952\n",
      "        37     -239132.4358          +0.3683\n",
      "        38     -239132.1626          +0.2731\n",
      "        39     -239132.0190          +0.1436\n",
      "        40     -239132.0271          -0.0081\n"
     ]
    }
   ],
   "source": [
    "cname = \"tien\"\n",
    "n_coms = 12\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mst',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class benh_nhan\n",
      "(6393, 39) [51, 39, 41, 47, 41, 65, 39, 52, 43, 48, 59, 68, 38, 50, 32, 40, 66, 50, 44, 41, 102, 44, 38, 49, 56, 44, 48, 33, 35, 54, 37, 38, 58, 32, 42, 61, 51, 47, 49, 40, 52, 31, 62, 56, 47, 35, 36, 58, 49, 39, 35, 40, 36, 42, 32, 34, 49, 53, 42, 40, 39, 45, 39, 40, 54, 50, 40, 58, 37, 46, 41, 33, 27, 27, 50, 45, 50, 52, 35, 45, 56, 37, 42, 65, 55, 42, 50, 37, 37, 45, 47, 269, 189, 155, 163, 145, 143, 114, 145, 130, 130, 140, 119, 153, 130, 122] 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -585177.1428             +nan\n",
      "         2     -501276.4420      +83900.7008\n",
      "         3     -485035.2964      +16241.1456\n",
      "         4     -478658.8578       +6376.4386\n",
      "         5     -475136.2451       +3522.6127\n",
      "         6     -473432.8812       +1703.3639\n",
      "         7     -472356.9839       +1075.8973\n",
      "         8     -471836.3168        +520.6670\n",
      "         9     -471433.9045        +402.4123\n",
      "        10     -471135.4950        +298.4095\n",
      "        11     -470811.0596        +324.4355\n",
      "        12     -470534.6113        +276.4483\n",
      "        13     -470387.6159        +146.9953\n",
      "        14     -470224.4878        +163.1281\n",
      "        15     -470158.2998         +66.1880\n",
      "        16     -470089.2130         +69.0868\n",
      "        17     -469993.6646         +95.5484\n",
      "        18     -469937.4669         +56.1977\n",
      "        19     -469884.0276         +53.4392\n",
      "        20     -469802.9333         +81.0943\n",
      "        21     -469707.5774         +95.3559\n",
      "        22     -469654.5636         +53.0139\n",
      "        23     -469602.9401         +51.6235\n",
      "        24     -469540.7578         +62.1822\n",
      "        25     -469483.6243         +57.1335\n",
      "        26     -469428.7919         +54.8324\n",
      "        27     -469388.4241         +40.3679\n",
      "        28     -469343.0474         +45.3766\n",
      "        29     -469302.5871         +40.4603\n",
      "        30     -469272.9079         +29.6792\n",
      "        31     -469256.2305         +16.6774\n",
      "        32     -469220.8273         +35.4033\n",
      "        33     -469160.4264         +60.4008\n",
      "        34     -469095.9888         +64.4377\n",
      "        35     -469060.3347         +35.6540\n",
      "        36     -469041.5176         +18.8171\n",
      "        37     -469031.4630         +10.0546\n",
      "        38     -469030.5688          +0.8942\n",
      "        39     -469031.2819          -0.7131\n"
     ]
    }
   ],
   "source": [
    "cname = \"benh_nhan\"\n",
    "n_coms = 24\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mst',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class y_te\n",
      "(6673, 39) [32, 44, 37, 50, 39, 44, 50, 39, 43, 63, 40, 62, 41, 47, 42, 37, 32, 44, 37, 50, 42, 49, 31, 46, 45, 34, 64, 58, 49, 72, 60, 59, 58, 39, 37, 49, 49, 39, 44, 50, 39, 60, 56, 52, 44, 61, 54, 39, 40, 76, 63, 37, 35, 51, 42, 41, 49, 41, 58, 59, 57, 38, 44, 53, 46, 54, 50, 46, 38, 50, 45, 47, 55, 57, 43, 38, 44, 35, 41, 58, 53, 38, 51, 61, 57, 48, 47, 63, 49, 34, 59, 43, 259, 173, 143, 155, 143, 135, 148, 143, 140, 140, 145, 135, 148, 132, 148] 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -602991.7395             +nan\n",
      "         2     -523344.1965      +79647.5430\n",
      "         3     -506510.0605      +16834.1360\n",
      "         4     -500989.8785       +5520.1820\n",
      "         5     -498679.0765       +2310.8020\n",
      "         6     -497775.9244        +903.1521\n",
      "         7     -497162.3950        +613.5294\n",
      "         8     -496757.8203        +404.5746\n",
      "         9     -496618.5643        +139.2560\n",
      "        10     -496563.6253         +54.9389\n",
      "        11     -496524.3702         +39.2552\n",
      "        12     -496484.9023         +39.4679\n",
      "        13     -496424.4600         +60.4423\n",
      "        14     -496377.4941         +46.9659\n",
      "        15     -496355.2805         +22.2136\n",
      "        16     -496347.3318          +7.9487\n",
      "        17     -496341.4112          +5.9205\n",
      "        18     -496335.9244          +5.4869\n",
      "        19     -496330.3123          +5.6121\n",
      "        20     -496326.1678          +4.1445\n",
      "        21     -496323.7972          +2.3706\n",
      "        22     -496322.2092          +1.5880\n",
      "        23     -496320.9833          +1.2259\n",
      "        24     -496319.9463          +1.0370\n",
      "        25     -496318.9840          +0.9623\n",
      "        26     -496317.9821          +1.0019\n",
      "        27     -496316.7539          +1.2282\n",
      "        28     -496315.0739          +1.6800\n",
      "        29     -496313.2773          +1.7966\n",
      "        30     -496311.3360          +1.9413\n",
      "        31     -496308.9158          +2.4202\n",
      "        32     -496304.7639          +4.1519\n",
      "        33     -496295.0050          +9.7589\n",
      "        34     -496279.1693         +15.8357\n",
      "        35     -496272.6470          +6.5222\n",
      "        36     -496268.3205          +4.3266\n",
      "        37     -496264.1768          +4.1437\n",
      "        38     -496258.6815          +5.4953\n",
      "        39     -496242.2655         +16.4160\n",
      "        40     -496219.9423         +22.3232\n",
      "        41     -496215.6100          +4.3323\n",
      "        42     -496211.7224          +3.8876\n",
      "        43     -496207.4650          +4.2574\n",
      "        44     -496204.0979          +3.3672\n",
      "        45     -496201.0296          +3.0683\n",
      "        46     -496198.7166          +2.3130\n",
      "        47     -496197.1753          +1.5414\n",
      "        48     -496195.9990          +1.1763\n",
      "        49     -496194.8387          +1.1603\n",
      "        50     -496193.3039          +1.5347\n",
      "        51     -496191.5899          +1.7140\n",
      "        52     -496190.5163          +1.0736\n",
      "        53     -496189.7736          +0.7428\n",
      "        54     -496189.1140          +0.6596\n",
      "        55     -496188.4063          +0.7077\n",
      "        56     -496187.5812          +0.8251\n",
      "        57     -496186.6916          +0.8896\n",
      "        58     -496185.8238          +0.8678\n",
      "        59     -496185.0853          +0.7385\n",
      "        60     -496184.6381          +0.4472\n",
      "        61     -496184.4450          +0.1931\n",
      "        62     -496184.3700          +0.0750\n",
      "        63     -496184.3378          +0.0322\n",
      "        64     -496184.3203          +0.0175\n",
      "        65     -496184.3080          +0.0123\n",
      "        66     -496184.2976          +0.0103\n",
      "        67     -496184.2882          +0.0095\n"
     ]
    }
   ],
   "source": [
    "cname = \"y_te\"\n",
    "n_coms = 9\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mst',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class duong\n",
      "(3201, 39) [19, 47, 23, 17, 19, 16, 36, 19, 19, 19, 21, 14, 17, 15, 35, 22, 36, 22, 16, 15, 24, 18, 16, 19, 26, 36, 38, 38, 35, 27, 24, 19, 47, 14, 32, 18, 15, 24, 27, 33, 17, 23, 19, 18, 17, 28, 20, 19, 16, 32, 21, 285, 114, 158, 124, 150, 106, 119, 132, 106, 114, 124, 109, 122, 101, 130] 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -265986.4212             +nan\n",
      "         2     -216422.9533      +49563.4680\n",
      "         3     -208758.6877       +7664.2655\n",
      "         4     -205861.1136       +2897.5742\n",
      "         5     -204386.1643       +1474.9493\n",
      "         6     -204041.7879        +344.3764\n",
      "         7     -203704.0040        +337.7839\n",
      "         8     -203443.9916        +260.0124\n",
      "         9     -203238.9421        +205.0495\n",
      "        10     -203033.7583        +205.1839\n",
      "        11     -202824.8886        +208.8697\n",
      "        12     -202705.5915        +119.2971\n",
      "        13     -202627.4280         +78.1635\n",
      "        14     -202529.6019         +97.8260\n",
      "        15     -202456.5651         +73.0369\n",
      "        16     -202430.5238         +26.0412\n",
      "        17     -202397.1550         +33.3689\n",
      "        18     -202353.2523         +43.9027\n",
      "        19     -202282.7088         +70.5435\n",
      "        20     -202219.1291         +63.5797\n",
      "        21     -202160.6808         +58.4483\n",
      "        22     -202099.0676         +61.6132\n",
      "        23     -202058.6717         +40.3959\n",
      "        24     -202021.0281         +37.6435\n",
      "        25     -201936.4077         +84.6204\n",
      "        26     -201884.6136         +51.7942\n",
      "        27     -201858.6069         +26.0066\n",
      "        28     -201837.6669         +20.9400\n",
      "        29     -201820.4917         +17.1751\n",
      "        30     -201814.2585          +6.2333\n",
      "        31     -201809.6701          +4.5884\n",
      "        32     -201800.0110          +9.6591\n",
      "        33     -201775.4123         +24.5987\n",
      "        34     -201755.0368         +20.3755\n",
      "        35     -201744.7636         +10.2732\n",
      "        36     -201740.8644          +3.8992\n",
      "        37     -201730.1430         +10.7214\n",
      "        38     -201724.4842          +5.6588\n",
      "        39     -201702.2666         +22.2176\n",
      "        40     -201668.5001         +33.7665\n",
      "        41     -201633.6942         +34.8059\n",
      "        42     -201629.1133          +4.5810\n",
      "        43     -201626.8839          +2.2293\n",
      "        44     -201624.7827          +2.1013\n",
      "        45     -201623.3257          +1.4569\n",
      "        46     -201622.1142          +1.2116\n",
      "        47     -201620.6005          +1.5136\n",
      "        48     -201619.8094          +0.7911\n",
      "        49     -201619.6258          +0.1836\n",
      "        50     -201619.5811          +0.0447\n",
      "        51     -201619.5715          +0.0096\n"
     ]
    }
   ],
   "source": [
    "cname = \"duong\"\n",
    "n_coms = 15\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mst',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hai': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'benh_nhan': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ....\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'y_te': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'duong': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ....\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'tien': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "         0.        , 0.        , 0.33333333, 0.33333333, 0.33333333,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]]))}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_y_te y_te\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_tien tien\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_duong duong\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "test_benh_nhan benh_nhan\n",
      "{'test_y_te': 1.0, 'test_hai': 1.0, 'test_tien': 1.0, 'test_duong': 1.0, 'test_benh_nhan': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing\")\n",
    "accuracy = {}\n",
    "test_name = {\"test_y_te\", \"test_hai\", \"test_benh_nhan\", \"test_duong\", \"test_tien\"}\n",
    "for true_cname in test_name:\n",
    "    k = 0\n",
    "    for O in dataset[true_cname]:\n",
    "        score = {cname : model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test' }\n",
    "        inverse = [(value, key) for key, value in score.items()]\n",
    "        predict = max(inverse)[1]\n",
    "        print(true_cname, predict)\n",
    "        if predict == true_cname[5:]:\n",
    "            k +=1\n",
    "    accuracy[true_cname] = k/len(dataset[true_cname])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"gmm_hmm1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit0d02a1b1d65743c3aa043d986ab8b5ed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
