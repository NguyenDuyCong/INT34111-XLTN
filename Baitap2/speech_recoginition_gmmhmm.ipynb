{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path) # read .wav file\n",
    "    yt, index = librosa.effects.trim(y) # Trim the beginning and ending silence\n",
    "    hop_length = math.floor(sr*0.010) # 10ms hop\n",
    "    win_length = math.floor(sr*0.025) # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        yt, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1,1)) \n",
    "    # energy feature\n",
    "    rms = librosa.feature.rms(y, hop_length=hop_length)\n",
    "    frame_feature = np.concatenate([mfcc, rms], axis=0)\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(frame_feature, order=1, mode='nearest')\n",
    "    delta2 = librosa.feature.delta(frame_feature, order=2, mode='nearest')\n",
    "    # X is 39 x T\n",
    "    X = np.concatenate([frame_feature, delta1, delta2], axis=0) # O^r\n",
    "    # return T x 39 (transpose of X)\n",
    "    return X.T # hmmlearn use T x N matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 39)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc = get_mfcc(\"./data/duong/17.wav\")\n",
    "mfcc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "#     print(files)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir,f)) for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load hai dataset\n",
      "Load tien dataset\n",
      "Load duong dataset\n",
      "Load y_te dataset\n",
      "Load benh_nhan dataset\n",
      "Load test_hai dataset\n",
      "Load test_tien dataset\n",
      "Load test_duong dataset\n",
      "Load test_y_te dataset\n",
      "Load test_benh_nhan dataset\n",
      "vectors (25089, 39)\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"hai\", \"tien\", \"duong\", \"y_te\", \"benh_nhan\", \"test_hai\", \"test_tien\", \"test_duong\", \"test_y_te\", \"test_benh_nhan\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)\n",
    "# Run K-Means algorithm to get clusters\n",
    "# kmeans = clustering(all_vectors)\n",
    "# print(\"centers\", kmeans.cluster_centers_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initByBakis(nStates, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nStates)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nStates, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "    \n",
    "def getTransmatPrior(nStates, bakisLevel):\n",
    "    ''' get transmat prior '''\n",
    "    transmatPrior = (1. / bakisLevel) * np.eye(nStates)\n",
    "\n",
    "    for i in range(nStates - (bakisLevel - 1)):\n",
    "        for j in range(bakisLevel - 1):\n",
    "            transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "\n",
    "    for i in range(nStates - bakisLevel + 1, nStates):\n",
    "        for j in range(nStates - i -j):\n",
    "            transmatPrior[i, i + j] = 1. / (nStates - i)\n",
    "\n",
    "    return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 0.5, 0. , 0. , 0. ]),\n",
       " array([[0.33333333, 0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 1.        ]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initByBakis(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class hai\n",
      "(2864, 39) [33, 31, 44, 40, 42, 18, 25, 26, 35, 44, 22, 35, 23, 35, 27, 27, 15, 19, 26, 15, 26, 23, 20, 16, 24, 33, 21, 15, 19, 20, 19, 23, 18, 15, 25, 23, 20, 19, 24, 16, 16, 22, 27, 27, 14, 14, 18, 20, 34, 13, 13, 13, 21, 14, 19, 16, 27, 22, 14, 28, 17, 12, 21, 18, 32, 17, 21, 17, 13, 17, 18, 13, 8, 16, 12, 15, 14, 10, 28, 18, 12, 25, 148, 130, 114, 143, 155, 132, 135, 140] 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -241816.8972             +nan\n",
      "         2     -207178.8953      +34638.0019\n",
      "         3     -200476.1197       +6702.7756\n",
      "         4     -198561.9008       +1914.2189\n",
      "         5     -197650.4699        +911.4309\n",
      "         6     -197316.7516        +333.7183\n",
      "         7     -197060.2493        +256.5023\n",
      "         8     -196706.5455        +353.7039\n",
      "         9     -196348.7183        +357.8272\n",
      "        10     -196130.2216        +218.4967\n",
      "        11     -195843.4886        +286.7330\n",
      "        12     -195619.8445        +223.6441\n",
      "        13     -195521.1575         +98.6870\n",
      "        14     -195482.1540         +39.0035\n",
      "        15     -195444.0988         +38.0553\n",
      "        16     -195428.1849         +15.9138\n",
      "        17     -195421.4228          +6.7621\n",
      "        18     -195414.0070          +7.4158\n",
      "        19     -195405.6877          +8.3193\n",
      "        20     -195400.1352          +5.5524\n",
      "        21     -195395.5577          +4.5775\n",
      "        22     -195390.7178          +4.8399\n",
      "        23     -195386.1527          +4.5651\n",
      "        24     -195381.9691          +4.1836\n",
      "        25     -195378.7112          +3.2579\n",
      "        26     -195376.1612          +2.5500\n",
      "        27     -195373.7209          +2.4403\n",
      "        28     -195371.2044          +2.5165\n",
      "        29     -195367.9226          +3.2819\n",
      "        30     -195361.9199          +6.0027\n",
      "        31     -195354.1758          +7.7441\n",
      "        32     -195346.1673          +8.0084\n",
      "        33     -195340.4096          +5.7577\n",
      "        34     -195335.3704          +5.0392\n",
      "        35     -195331.0383          +4.3321\n",
      "        36     -195328.0662          +2.9721\n",
      "        37     -195327.0327          +1.0335\n",
      "        38     -195326.7153          +0.3174\n",
      "        39     -195326.5164          +0.1989\n",
      "        40     -195326.3608          +0.1556\n",
      "        41     -195326.2435          +0.1173\n",
      "        42     -195326.1648          +0.0787\n",
      "        43     -195326.1140          +0.0508\n",
      "        44     -195326.0775          +0.0365\n",
      "        45     -195326.0470          +0.0305\n",
      "        46     -195326.0182          +0.0288\n",
      "        47     -195325.9876          +0.0305\n",
      "        48     -195325.9507          +0.0369\n",
      "        49     -195325.8999          +0.0508\n",
      "        50     -195325.8264          +0.0735\n",
      "        51     -195325.7344          +0.0919\n",
      "        52     -195325.6562          +0.0782\n",
      "        53     -195325.6128          +0.0434\n",
      "        54     -195325.5907          +0.0220\n",
      "        55     -195325.5770          +0.0138\n",
      "        56     -195325.5668          +0.0101\n",
      "        57     -195325.5587          +0.0082\n"
     ]
    }
   ],
   "source": [
    "cname = \"hai\"\n",
    "n_coms = 11\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=4)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mcw',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class tien\n",
      "(3602, 39) [13, 14, 18, 19, 16, 18, 24, 21, 16, 20, 17, 24, 18, 26, 19, 16, 16, 29, 24, 28, 19, 36, 25, 26, 29, 24, 23, 31, 29, 25, 14, 24, 18, 18, 19, 18, 16, 19, 24, 16, 19, 13, 21, 55, 17, 28, 25, 20, 41, 23, 24, 20, 14, 13, 24, 29, 41, 26, 26, 36, 25, 18, 25, 18, 28, 18, 19, 19, 24, 55, 29, 26, 24, 21, 16, 31, 24, 19, 17, 16, 28, 25, 23, 16, 104, 104, 99, 109, 109, 112, 112, 114, 124, 106, 117, 112, 124, 114, 114] 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -311532.6442             +nan\n",
      "         2     -261133.1728      +50399.4714\n",
      "         3     -252666.4948       +8466.6780\n",
      "         4     -248827.5666       +3838.9282\n",
      "         5     -247429.5097       +1398.0569\n",
      "         6     -246982.8152        +446.6945\n",
      "         7     -246714.6722        +268.1430\n",
      "         8     -246433.9285        +280.7437\n",
      "         9     -246078.1939        +355.7347\n",
      "        10     -245830.9455        +247.2483\n",
      "        11     -245778.2100         +52.7356\n",
      "        12     -245744.1644         +34.0455\n",
      "        13     -245707.6596         +36.5048\n",
      "        14     -245657.8353         +49.8244\n",
      "        15     -245634.8292         +23.0061\n",
      "        16     -245607.3066         +27.5226\n",
      "        17     -245584.8259         +22.4806\n",
      "        18     -245577.9783          +6.8476\n",
      "        19     -245575.1715          +2.8068\n",
      "        20     -245573.0201          +2.1515\n",
      "        21     -245570.2104          +2.8097\n",
      "        22     -245566.5038          +3.7066\n",
      "        23     -245564.2683          +2.2356\n",
      "        24     -245563.4475          +0.8208\n",
      "        25     -245562.4974          +0.9501\n",
      "        26     -245561.2648          +1.2326\n",
      "        27     -245560.4516          +0.8132\n",
      "        28     -245559.9625          +0.4891\n",
      "        29     -245559.1166          +0.8459\n",
      "        30     -245557.2035          +1.9131\n",
      "        31     -245555.4740          +1.7295\n",
      "        32     -245554.8599          +0.6141\n",
      "        33     -245554.4092          +0.4507\n",
      "        34     -245553.9613          +0.4480\n",
      "        35     -245553.6972          +0.2641\n",
      "        36     -245553.6366          +0.0606\n",
      "        37     -245553.5880          +0.0486\n",
      "        38     -245553.3776          +0.2103\n",
      "        39     -245552.3527          +1.0250\n",
      "        40     -245549.4059          +2.9468\n",
      "        41     -245544.4357          +4.9702\n",
      "        42     -245529.3522         +15.0834\n",
      "        43     -245526.2559          +3.0963\n",
      "        44     -245525.7323          +0.5236\n",
      "        45     -245525.6445          +0.0878\n",
      "        46     -245525.5392          +0.1053\n",
      "        47     -245525.2341          +0.3051\n",
      "        48     -245523.4992          +1.7349\n",
      "        49     -245517.8787          +5.6205\n",
      "        50     -245516.7696          +1.1091\n",
      "        51     -245516.5313          +0.2383\n",
      "        52     -245516.3713          +0.1600\n",
      "        53     -245516.3227          +0.0486\n",
      "        54     -245516.3143          +0.0083\n"
     ]
    }
   ],
   "source": [
    "cname = \"tien\"\n",
    "n_coms = 9\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mcw',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class benh_nhan\n",
      "(6393, 39) [51, 39, 41, 47, 41, 65, 39, 52, 43, 48, 59, 68, 38, 50, 32, 40, 66, 50, 44, 41, 102, 44, 38, 49, 56, 44, 48, 33, 35, 54, 37, 38, 58, 32, 42, 61, 51, 47, 49, 40, 52, 31, 62, 56, 47, 35, 36, 58, 49, 39, 35, 40, 36, 42, 32, 34, 49, 53, 42, 40, 39, 45, 39, 40, 54, 50, 40, 58, 37, 46, 41, 33, 27, 27, 50, 45, 50, 52, 35, 45, 56, 37, 42, 65, 55, 42, 50, 37, 37, 45, 47, 269, 189, 155, 163, 145, 143, 114, 145, 130, 130, 140, 119, 153, 130, 122] 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -588334.5662             +nan\n",
      "         2     -508378.6066      +79955.9596\n",
      "         3     -493018.9806      +15359.6259\n",
      "         4     -488086.3278       +4932.6528\n",
      "         5     -485039.6500       +3046.6778\n",
      "         6     -483660.7469       +1378.9032\n",
      "         7     -482901.1909        +759.5560\n",
      "         8     -482311.3987        +589.7921\n",
      "         9     -481746.9361        +564.4626\n",
      "        10     -481281.6224        +465.3138\n",
      "        11     -480924.9314        +356.6909\n",
      "        12     -480604.5054        +320.4260\n",
      "        13     -480355.2027        +249.3027\n",
      "        14     -480234.4162        +120.7865\n",
      "        15     -480121.2666        +113.1496\n",
      "        16     -479971.5449        +149.7217\n",
      "        17     -479844.7086        +126.8363\n",
      "        18     -479767.1000         +77.6086\n",
      "        19     -479682.4185         +84.6815\n",
      "        20     -479573.8465        +108.5720\n",
      "        21     -479409.5941        +164.2524\n",
      "        22     -479263.7876        +145.8065\n",
      "        23     -479148.5961        +115.1915\n",
      "        24     -479045.9536        +102.6425\n",
      "        25     -478986.8221         +59.1315\n",
      "        26     -478925.5698         +61.2523\n",
      "        27     -478865.0007         +60.5690\n",
      "        28     -478799.5499         +65.4509\n",
      "        29     -478725.0665         +74.4833\n",
      "        30     -478645.4429         +79.6236\n",
      "        31     -478554.3608         +91.0821\n",
      "        32     -478475.5415         +78.8193\n",
      "        33     -478405.0725         +70.4690\n",
      "        34     -478229.3505        +175.7221\n",
      "        35     -478122.2279        +107.1225\n",
      "        36     -478094.1135         +28.1144\n",
      "        37     -478067.6933         +26.4203\n",
      "        38     -478048.6596         +19.0336\n",
      "        39     -478026.7486         +21.9110\n",
      "        40     -477987.4093         +39.3394\n",
      "        41     -477969.4280         +17.9813\n",
      "        42     -477926.0726         +43.3554\n",
      "        43     -477895.5878         +30.4848\n",
      "        44     -477871.8164         +23.7714\n",
      "        45     -477857.1410         +14.6754\n",
      "        46     -477846.2273         +10.9137\n",
      "        47     -477826.8583         +19.3689\n",
      "        48     -477805.7062         +21.1521\n",
      "        49     -477797.5648          +8.1414\n",
      "        50     -477780.5977         +16.9671\n",
      "        51     -477766.1749         +14.4228\n",
      "        52     -477754.6770         +11.4979\n",
      "        53     -477741.6798         +12.9972\n",
      "        54     -477714.5522         +27.1275\n",
      "        55     -477689.0056         +25.5466\n",
      "        56     -477674.8816         +14.1240\n",
      "        57     -477665.7848          +9.0968\n",
      "        58     -477659.1293          +6.6555\n",
      "        59     -477654.7703          +4.3590\n",
      "        60     -477651.3409          +3.4293\n",
      "        61     -477648.7240          +2.6169\n",
      "        62     -477646.0474          +2.6766\n",
      "        63     -477639.6098          +6.4377\n",
      "        64     -477636.9507          +2.6591\n",
      "        65     -477635.4574          +1.4933\n",
      "        66     -477634.4345          +1.0229\n",
      "        67     -477633.8133          +0.6212\n",
      "        68     -477633.3572          +0.4561\n",
      "        69     -477632.9527          +0.4045\n",
      "        70     -477632.5481          +0.4045\n",
      "        71     -477632.1058          +0.4423\n",
      "        72     -477631.5511          +0.5547\n",
      "        73     -477630.7186          +0.8326\n",
      "        74     -477629.3735          +1.3450\n",
      "        75     -477627.4024          +1.9711\n",
      "        76     -477625.0527          +2.3497\n",
      "        77     -477622.8471          +2.2057\n",
      "        78     -477621.2766          +1.5705\n",
      "        79     -477620.2833          +0.9932\n",
      "        80     -477619.6221          +0.6613\n",
      "        81     -477619.1538          +0.4683\n",
      "        82     -477618.7869          +0.3669\n",
      "        83     -477618.4371          +0.3498\n",
      "        84     -477617.9892          +0.4479\n",
      "        85     -477617.2593          +0.7299\n",
      "        86     -477616.2367          +1.0226\n",
      "        87     -477615.3796          +0.8571\n",
      "        88     -477614.7172          +0.6624\n",
      "        89     -477614.0233          +0.6939\n",
      "        90     -477613.1810          +0.8423\n",
      "        91     -477611.9204          +1.2606\n",
      "        92     -477608.6388          +3.2816\n",
      "        93     -477606.8543          +1.7845\n",
      "        94     -477606.3653          +0.4890\n",
      "        95     -477606.1095          +0.2558\n",
      "        96     -477605.9848          +0.1247\n",
      "        97     -477605.9258          +0.0590\n",
      "        98     -477605.8936          +0.0322\n",
      "        99     -477605.8726          +0.0210\n",
      "       100     -477605.8566          +0.0160\n",
      "       101     -477605.8428          +0.0138\n",
      "       102     -477605.8291          +0.0137\n",
      "       103     -477605.8133          +0.0158\n",
      "       104     -477605.7923          +0.0210\n",
      "       105     -477605.7600          +0.0323\n",
      "       106     -477605.7043          +0.0557\n",
      "       107     -477605.6047          +0.0995\n",
      "       108     -477605.4503          +0.1544\n",
      "       109     -477605.2885          +0.1619\n",
      "       110     -477605.1845          +0.1039\n",
      "       111     -477605.1314          +0.0532\n",
      "       112     -477605.1021          +0.0293\n",
      "       113     -477605.0838          +0.0183\n",
      "       114     -477605.0713          +0.0124\n",
      "       115     -477605.0624          +0.0089\n"
     ]
    }
   ],
   "source": [
    "cname = \"benh_nhan\"\n",
    "n_coms = 18\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mcw',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class y_te\n",
      "(6673, 39) [32, 44, 37, 50, 39, 44, 50, 39, 43, 63, 40, 62, 41, 47, 42, 37, 32, 44, 37, 50, 42, 49, 31, 46, 45, 34, 64, 58, 49, 72, 60, 59, 58, 39, 37, 49, 49, 39, 44, 50, 39, 60, 56, 52, 44, 61, 54, 39, 40, 76, 63, 37, 35, 51, 42, 41, 49, 41, 58, 59, 57, 38, 44, 53, 46, 54, 50, 46, 38, 50, 45, 47, 55, 57, 43, 38, 44, 35, 41, 58, 53, 38, 51, 61, 57, 48, 47, 63, 49, 34, 59, 43, 259, 173, 143, 155, 143, 135, 148, 143, 140, 140, 145, 135, 148, 132, 148] 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -602953.7082             +nan\n",
      "         2     -523607.9579      +79345.7503\n",
      "         3     -506833.3518      +16774.6061\n",
      "         4     -501076.1660       +5757.1858\n",
      "         5     -498573.5139       +2502.6521\n",
      "         6     -497642.4728        +931.0411\n",
      "         7     -497037.7578        +604.7150\n",
      "         8     -496691.1885        +346.5693\n",
      "         9     -496592.2840         +98.9045\n",
      "        10     -496539.8678         +52.4162\n",
      "        11     -496479.6764         +60.1914\n",
      "        12     -496386.8359         +92.8405\n",
      "        13     -496365.3436         +21.4923\n",
      "        14     -496353.7188         +11.6248\n",
      "        15     -496346.5129          +7.2059\n",
      "        16     -496342.0716          +4.4413\n",
      "        17     -496338.8722          +3.1994\n",
      "        18     -496336.2461          +2.6261\n",
      "        19     -496333.7857          +2.4603\n",
      "        20     -496331.1064          +2.6793\n",
      "        21     -496327.1101          +3.9963\n",
      "        22     -496315.6478         +11.4623\n",
      "        23     -496300.3854         +15.2624\n",
      "        24     -496280.7238         +19.6616\n",
      "        25     -496259.8213         +20.9025\n",
      "        26     -496252.9901          +6.8312\n",
      "        27     -496248.8136          +4.1765\n",
      "        28     -496245.9914          +2.8222\n",
      "        29     -496244.0476          +1.9438\n",
      "        30     -496242.6883          +1.3593\n",
      "        31     -496241.7113          +0.9770\n",
      "        32     -496240.9823          +0.7290\n",
      "        33     -496240.4120          +0.5703\n",
      "        34     -496239.9319          +0.4800\n",
      "        35     -496239.4628          +0.4691\n",
      "        36     -496238.7582          +0.7047\n",
      "        37     -496236.2041          +2.5541\n",
      "        38     -496232.3089          +3.8952\n",
      "        39     -496230.7718          +1.5370\n",
      "        40     -496229.7059          +1.0659\n",
      "        41     -496228.7371          +0.9688\n",
      "        42     -496227.7422          +0.9949\n",
      "        43     -496226.7019          +1.0403\n",
      "        44     -496225.6587          +1.0433\n",
      "        45     -496224.7135          +0.9452\n",
      "        46     -496223.9673          +0.7461\n",
      "        47     -496223.4115          +0.5558\n",
      "        48     -496222.9695          +0.4420\n",
      "        49     -496222.6099          +0.3596\n",
      "        50     -496222.3415          +0.2684\n",
      "        51     -496222.1605          +0.1810\n",
      "        52     -496222.0431          +0.1174\n",
      "        53     -496221.9669          +0.0762\n",
      "        54     -496221.9170          +0.0499\n",
      "        55     -496221.8841          +0.0329\n",
      "        56     -496221.8622          +0.0219\n",
      "        57     -496221.8475          +0.0147\n",
      "        58     -496221.8376          +0.0099\n"
     ]
    }
   ],
   "source": [
    "cname = \"y_te\"\n",
    "n_coms = 9\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mcw',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class duong\n",
      "(3201, 39) [19, 47, 23, 17, 19, 16, 36, 19, 19, 19, 21, 14, 17, 15, 35, 22, 36, 22, 16, 15, 24, 18, 16, 19, 26, 36, 38, 38, 35, 27, 24, 19, 47, 14, 32, 18, 15, 24, 27, 33, 17, 23, 19, 18, 17, 28, 20, 19, 16, 32, 21, 285, 114, 158, 124, 150, 106, 119, 132, 106, 114, 124, 109, 122, 101, 130] 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -268263.9947             +nan\n",
      "         2     -224835.6012      +43428.3935\n",
      "         3     -216918.8749       +7916.7263\n",
      "         4     -214385.9551       +2532.9198\n",
      "         5     -213377.4732       +1008.4818\n",
      "         6     -212802.6105        +574.8627\n",
      "         7     -212587.3615        +215.2490\n",
      "         8     -212447.3338        +140.0277\n",
      "         9     -212319.9539        +127.3799\n",
      "        10     -212220.8676         +99.0863\n",
      "        11     -212123.0911         +97.7765\n",
      "        12     -212034.5731         +88.5180\n",
      "        13     -211946.9150         +87.6581\n",
      "        14     -211873.2184         +73.6966\n",
      "        15     -211831.9523         +41.2661\n",
      "        16     -211807.8871         +24.0653\n",
      "        17     -211799.4484          +8.4386\n",
      "        18     -211795.4766          +3.9718\n",
      "        19     -211790.9047          +4.5719\n",
      "        20     -211783.6397          +7.2650\n",
      "        21     -211762.6371         +21.0026\n",
      "        22     -211740.9028         +21.7343\n",
      "        23     -211726.3763         +14.5265\n",
      "        24     -211711.6939         +14.6823\n",
      "        25     -211706.8956          +4.7984\n",
      "        26     -211704.6343          +2.2613\n",
      "        27     -211704.0801          +0.5542\n",
      "        28     -211703.8248          +0.2553\n",
      "        29     -211703.6954          +0.1294\n",
      "        30     -211703.6187          +0.0767\n",
      "        31     -211703.5615          +0.0572\n",
      "        32     -211703.5084          +0.0531\n",
      "        33     -211703.4503          +0.0582\n",
      "        34     -211703.3784          +0.0718\n",
      "        35     -211703.2823          +0.0961\n",
      "        36     -211703.1474          +0.1349\n",
      "        37     -211702.9530          +0.1944\n",
      "        38     -211702.6631          +0.2899\n",
      "        39     -211702.2018          +0.4613\n",
      "        40     -211701.3931          +0.8087\n",
      "        41     -211699.6883          +1.7048\n",
      "        42     -211696.4630          +3.2253\n",
      "        43     -211694.5765          +1.8865\n",
      "        44     -211693.5688          +1.0077\n",
      "        45     -211692.8070          +0.7618\n",
      "        46     -211692.3631          +0.4439\n",
      "        47     -211692.1664          +0.1967\n",
      "        48     -211692.0874          +0.0790\n",
      "        49     -211692.0561          +0.0313\n",
      "        50     -211692.0430          +0.0131\n",
      "        51     -211692.0368          +0.0062\n"
     ]
    }
   ],
   "source": [
    "cname = \"duong\"\n",
    "n_coms = 9\n",
    "startprobPrior,transmatPrior = initByBakis(nStates=n_coms,bakisLevel=3)\n",
    "hmm = hmmlearn.hmm.GMMHMM(\n",
    "    n_components = n_coms, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "    params='mctw',\n",
    "    init_params='mcw',\n",
    "    startprob_prior = startprobPrior,\n",
    "    transmat_prior = transmatPrior\n",
    ")\n",
    "\n",
    "X = np.concatenate(dataset[cname])\n",
    "lengths = list([len(x) for x in dataset[cname]])\n",
    "print(\"training class\", cname)\n",
    "print(X.shape, lengths, len(lengths))\n",
    "hmm.fit(X)\n",
    "models[cname] = hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hai': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "         0.25      , 0.25      , 0.25      , 0.25      , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.25      , 0.25      , 0.25      , 0.25      ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.25      , 0.25      , 0.25      ,\n",
       "         0.25      ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "         0.5       ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.25      ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'tien': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'benh_nhan': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, ....\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'y_te': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])),\n",
       " 'duong': GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "        covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5],\n",
       "         [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
       "          -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -...\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "         0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        ]]),\n",
       "        verbose=True,\n",
       "        weights_prior=array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]]))}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "test_hai hai\n",
      "{'test_hai': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing\")\n",
    "accuracy = {}\n",
    "test_name = {\"test_hai\"}\n",
    "for true_cname in test_name:\n",
    "    k = 0\n",
    "    for O in dataset[true_cname]:\n",
    "        score = {cname : model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test' }\n",
    "#         print(score)\n",
    "        inverse = [(value, key) for key, value in score.items()]\n",
    "        predict = max(inverse)[1]\n",
    "        print(true_cname, predict)\n",
    "        if predict == true_cname[5:]:\n",
    "            k +=1\n",
    "    accuracy[true_cname] = k/len(dataset[true_cname])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"gmm_hmm3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit0d02a1b1d65743c3aa043d986ab8b5ed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
